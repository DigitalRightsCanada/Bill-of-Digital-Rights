# Bill-of-Digital-Rights

Bill C-0101

The Canadian Digital Freedom and Rights Act


---

Your Data. Your Rights. Your Future.


---

âœ… You Own Your Data

Your information is your property.

Nobody can use, sell, or track it without your clear consent.


âœ… Fair Value

If your data makes money, you get a fair share.


âœ… Transparency

You have the right to know who has your data, why, and for how long.

No more hidden fine print or dark tricks.


âœ… Control

Download your data.

Move it anywhere.

Delete it whenever you want.


âœ… Protection

Algorithms canâ€™t secretly decide your future.

No discrimination.

No targeting kids.

Extra safeguards for biometrics and brain data.



---

What Bill C-0101 Stops

ğŸš« Forcing you to give up data for essential services.
ğŸš« Secret shadow profiles.
ğŸš« Selling your data without paying you.
ğŸš« Manipulative consent tricks.
ğŸš« Exploiting your weaknesses (addiction, gambling, manipulation).


---

Accountability That Counts

Independent Digital Rights Ombudsperson.

Digital Rights Tribunal with real enforcement powers.

Penalties: up to $25M or 5% of global revenue.

Serious violations: up to $50M + jail time.



---

Future-Proofed

Covers todayâ€™s tech â€” and tomorrowâ€™s.
High-risk systems like facial recognition, predictive policing, child profiling, and brain-computer tech get strict rules.


---

Why It Matters

Bill C-0101 makes sure the digital world works for people, not just corporations or the state.
It protects our freedom, dignity, and fairness in the 21st century.


---

ğŸ“¢ Your data belongs to you.
This is Canadaâ€™s chance to set the standard for the world.

-
BILL C-0101

An Act to recognize and protect the digital rights and freedoms of individuals in Canada, to establish ownership and fair value of personal data, and to provide transparency and accountability in digital practices


---

Preamble

Whereas Parliament affirms that personal data is an extension of individual autonomy and dignity;

Whereas the fair and transparent use of data is necessary to preserve democracy, human rights, and equality of opportunity;

And whereas it is expedient to establish a modern digital framework that prevents overreach, protects individuals against exploitation, and ensures accountability of both public and private actors;

Now, therefore, Her Majesty, by and with the advice and consent of the Senate and House of Commons of Canada, enacts as follows:


---

Short Title

Short title

1. This Act may be cited as the Canadian Digital Freedom and Rights Act.




---

PART I â€” PURPOSE AND INTERPRETATION

Purpose
2. The purpose of this Act is to guarantee individual ownership and control of personal data, to ensure fair sharing of value, to require transparency and accountability in digital practices, and to protect the dignity and autonomy of all individuals in Canada.

Definitions
3. The following definitions apply in this Act:

(a) â€œalgorithmic systemâ€ means any automated process, including artificial intelligence or machine learning, that uses personal data to inform or make decisions;
(b) â€œbiometric dataâ€ means data relating to physical, physiological or behavioural characteristics that uniquely identify an individual;
(c) â€œCommissionerâ€ means the Digital Rights Ombudsperson appointed under Part VII;
(d) â€œconsentâ€ means clear, informed, specific and freely given permission by an individual that may be withdrawn at any time without penalty or loss of essential services;
(e) â€œdark patternâ€ means any design or process intended to manipulate an individual into surrendering personal data or rights;
(f) â€œdata monetizationâ€ means any sale, licensing, transfer or commercial use of personal data;
(g) â€œentityâ€ means any government institution, corporation, partnership, association or person engaged in the processing of personal data;
(h) â€œneurodataâ€ means data derived from neurological activity or cognitive processes;
(i) â€œpersonal dataâ€ means any information relating to an identifiable individual, including identifiers, communications, metadata, biometric data, neurodata, preferences, reputations, behavioural patterns and histories;
(j) â€œprocessingâ€ means any operation performed on personal data, including collection, recording, analysis, storage, transfer, disclosure, deletion and monetization.


---

PART II â€” RIGHTS OF INDIVIDUALS

Ownership of personal data
4. (1) Every individual is the sole owner of their personal data.
(2) No entity shall process personal data without the consent of the individual, except as required by law and subject to oversight under this Act.

Right to fair value
5. (1) No entity shall profit from personal data without providing fair compensation to the individual.
(2) Every individual is entitled to participate in any aggregate monetization scheme, including through a data dividend.

Right to transparency
6. (1) Every individual has the right, on request, to know in plain language
Â Â Â Â (a) what personal data is collected;
Â Â Â Â (b) by whom;
Â Â Â Â (c) for what purpose;
Â Â Â Â (d) for how long; and
Â Â Â Â (e) whether it has been monetized or transferred.
(2) No entity shall conceal its processing practices through complexity or obscurity.

Right to portability
7. (1) Every individual has the right to receive their personal data in a portable, standardized format.
(2) This right includes reputational value, transaction history and contribution records.

Right to erasure and expiry
8. (1) Every individual has the right to require deletion of their personal data.
(2) All personal data shall carry an expiry date after which it must be deleted unless consent is renewed.

Right to algorithmic fairness
9. (1) Every individual has the right to know when a decision is made or informed by an algorithmic system.
(2) No individual shall be subject to a decision based solely on automated processing without meaningful human review.
(3) Every individual may opt out of algorithmic processing where feasible.

Right to security and non-exploitation
10. (1) Every individual has the right to expect safeguards against unauthorized access.
(2) No entity shall process personal data for the purpose of exploiting vulnerabilities, including addiction, gambling or predatory targeting.

Right to non-discrimination
11. (1) No individual shall be denied services, employment or opportunities because they withhold personal data.
(2) No algorithmic system shall discriminate on grounds prohibited by the Canadian Human Rights Act.


---

PART III â€” PROHIBITED PRACTICES

Prohibitions
12. No entity shall
(a) require the surrender of personal data as a condition of essential services;
(b) employ dark patterns to obtain consent;
(c) create or sell shadow profiles on non-users;
(d) retain personal data beyond its expiry without renewed consent;
(e) monetize personal data without disclosure and compensation;
(f) collect or process biometric data or neurodata without explicit, separate consent.


---

PART IV â€” DUTIES OF ENTITIES

Duty of transparency
13. Entities shall maintain public records of their processing practices in plain language.

Duty of minimal collection
14. Entities shall collect only personal data necessary for the stated purpose at the time of collection.

Duty of fair compensation
15. Entities shall provide mechanisms for individuals to receive value from data monetization, whether through direct payment, benefit credits or dividends.

Duty of breach notification
16. Entities shall notify affected individuals and the Commissioner without delay of any unauthorized access, disclosure or misuse.

Duty of algorithmic audit
17. Entities deploying high-risk algorithms shall be subject to independent third-party audits for fairness and bias.


---

PART V â€” TRANSPARENCY AND OVERSIGHT

Open data ledger
18. (1) All monetization of personal data shall be recorded in a secure, auditable open data ledger.
(2) Entities shall disclose the value assigned to personal data in each transaction.
(3) Individuals may access aggregate information regarding the use of their data.


---

PART VI â€” SPECIAL PROTECTIONS

Biometric and neurodata
19. (1) Biometric data and neurodata are classified as highly sensitive.
(2) Their collection or processing requires explicit, separate consent.
(3) Such data shall not be monetized except through direct licensing by the individual.

Children
20. (1) The rights of minors under this Act shall be exercised by their guardians.
(2) On reaching the age of majority, an individual shall acquire full control of their personal data.
(3) Targeted advertising to minors is prohibited.


---

PART VII â€” OVERSIGHT AND REMEDIES

Office established
21. (1) There is established the Office of the Digital Rights Ombudsperson.
(2) The Ombudsperson is independent and reports directly to Parliament.
(3) The Ombudsperson may investigate complaints, mediate disputes, issue binding orders, and publish audits.

Tribunal
21.1 (1) A Digital Rights Tribunal is hereby established to hear appeals from orders of the Ombudsperson.
(2) The Tribunal has all the powers of a superior court of record.
(3) Decisions of the Tribunal may be appealed to the Federal Court of Appeal on questions of law.

Individual remedies
22. (1) Any person whose rights under this Act are infringed is entitled to restitution, compensation, and correction of records.
(2) Class proceedings may be brought for systemic violations.
(3) Punitive damages may be awarded where violations are deliberate or reckless.

Whistleblower protections
22.1 (1) No person shall be disciplined, dismissed, or otherwise disadvantaged for disclosing a contravention of this Act.
(2) Whistleblowers are entitled to confidentiality and protection against retaliation.


---

PART VIII â€” GOVERNMENT AND PUBLIC SECTOR OBLIGATIONS

Equal application
23. This Act binds the Crown.

Limits on surveillance
24. (1) No government institution shall collect, retain, or use personal data except as expressly authorized by law and consistent with this Act.
(2) Mass surveillance of individuals without individualized lawful authority is prohibited.
(3) Emergency powers under other statutes shall not be interpreted to override this Act, except as expressly provided by Parliament.

Foreign transfers
25. (1) No personal data shall be transferred outside Canada without notice to the individual.
(2) Entities shall disclose the jurisdiction and applicable legal regime of the receiving state.
(3) Individuals may withhold consent to foreign transfer.

National security exceptions
26. (1) Nothing in this Act prevents disclosure required under the Security of Canada Information Disclosure Act or the Criminal Code.
(2) Such disclosures must be reported annually to Parliament in aggregate form, except where public disclosure would endanger national security.
(3) Oversight of national security disclosures rests with the National Security and Intelligence Review Agency.


---

PART IX â€” ENFORCEMENT AND PENALTIES

Administrative penalties
27. (1) An entity that contravenes this Act is liable to an administrative monetary penalty not exceeding
Â Â Â Â (a) $10,000,000 or 2% of global gross revenue, whichever is greater, for a first contravention; and
Â Â Â Â (b) $25,000,000 or 5% of global gross revenue, whichever is greater, for a subsequent contravention.

Criminal offences
28. (1) Every person who knowingly
Â Â Â Â (a) monetizes personal data without consent,
Â Â Â Â (b) engages in mass surveillance contrary to section 24, or
Â Â Â Â (c) retaliates against a whistleblower,
is guilty of an offence punishable on conviction on indictment by a fine not exceeding $50,000,000 or imprisonment for a term not exceeding five years, or both.


---

PART X â€” ACCOUNTABILITY AND REVIEW

Annual report
29. The Ombudsperson shall table an annual report in both Houses of Parliament setting out compliance rates, systemic issues, and recommendations.

Parliamentary review
30. (1) A comprehensive review of this Act shall be undertaken every five years by a committee of the House of Commons designated for that purpose.
(2) The committee shall report to Parliament with recommendations for amendment.

Sunset of exceptions
31. Any exception to this Act enacted under section 26 shall expire five years after its enactment unless renewed by Parliament.


---

PART XI â€” GENERAL

Liberal interpretation
32. This Act shall be interpreted broadly and liberally in favour of the freedom, dignity, and autonomy of individuals.

Supremacy of Act
33. In the event of conflict with another Act of Parliament, this Act prevails to the extent of the conflict, except where the Canadian Charter of Rights and Freedoms otherwise provides.


---

PART XII â€” FUTURE-PROOFING DIGITAL RIGHTS

Technological neutrality
34. This Act applies to all technologies, platforms, and processing methods, whether existing or developed in the future.

Emerging technologies
35. (1) The Governor in Council may, on the recommendation of the Ombudsperson and following consultation with Parliament, make regulations designating new categories of high-risk algorithmic systems subject to this Act.
(2) Designated systems shall be added to the Schedule.
(3) The designation of a new category shall be reviewed within three years of its addition.

Precautionary principle
36. Where doubt exists as to the safety, fairness, or ethical impact of an algorithmic system, it shall be regulated as high-risk until proven otherwise.

Independent advisory panel
37. (1) There is established a Digital Futures Advisory Panel, composed of experts in technology, law, human rights, and ethics.
(2) The Panel shall publish biennial reports on emerging risks and recommend amendments to this Act.


---

SCHEDULE â€” HIGH-RISK ALGORITHMIC SYSTEMS

(Section 35)

The following categories of algorithmic systems are designated as high-risk and subject to enhanced obligations under this Act:

1. Biometric recognition systems â€” including facial, voice, gait, and iris recognition.


2. Predictive policing and surveillance systems â€” including crime prediction, risk scoring, and automated surveillance analytics.


3. Employment and labour systems â€” including algorithmic hiring, performance evaluation, and termination decisions.


4. Credit, insurance, and financial profiling systems â€” including automated credit scoring, loan approval, and premium calculation.


5. Health and genetic systems â€” including predictive health analytics, genetic risk scoring, and treatment recommendation engines.


6. Education and child profiling systems â€” including automated grading, behavioural monitoring, and opportunity allocation.


7. Content recommendation and behavioural manipulation systems â€” including algorithmic feeds, political micro-targeting, and addictive engagement loops.


8. Neurotechnology systems â€” including brainâ€“computer interfaces, cognitive monitoring, and emotional recognition.


9. Any system designated by regulation under section 35.




---

ANNEX â€” DRAFT REGULATIONS

(Pursuant to section 35 of the Canadian Digital Freedom and Rights Act)

Regulation 1 â€” Algorithmic Audit Standards

1. Every audit of a high-risk algorithmic system shall include:
Â Â Â Â (a) a description of the system, its purpose, and decision-making process;
Â Â Â Â (b) the categories of personal data used;
Â Â Â Â (c) the criteria, variables, or proxies influencing outputs;
Â Â Â Â (d) testing for bias, discrimination, and disparate impact on protected groups;
Â Â Â Â (e) a record of error rates and reliability measures;
Â Â Â Â (f) procedures for human review and override;
Â Â Â Â (g) a statement of compliance with the Canadian Human Rights Act.


2. Audits must be conducted at least annually by an independent, accredited third party.



Regulation 2 â€” Plain-Language Disclosures

1. Disclosures under section 6 of the Act must be written in plain language, not exceeding a Grade 8 reading level.


2. A disclosure must include:
Â Â Â Â (a) the name and contact information of the entity;
Â Â Â Â (b) the purpose of data collection;
Â Â Â Â (c) the categories of data collected;
Â Â Â Â (d) whether the data will be monetized, shared, or transferred abroad;
Â Â Â Â (e) the expiry date of the data;
Â Â Â Â (f) the rights of the individual under this Act.


3. No disclosure is valid if presented in a manipulative or coercive design (dark pattern).



Regulation 3 â€” Data Ledger Reporting

1. Entities monetizing personal data must submit quarterly reports to the Ombudsperson containing:
Â Â Â Â (a) total volume of data monetized;
Â Â Â Â (b) categories of data monetized;
Â Â Â Â (c) aggregate value received;
Â Â Â Â (d) proportion of value distributed to individuals;
Â Â Â Â (e) identity of third-party recipients.


2. Reports must be published in an anonymized public ledger accessible online.



Regulation 4 â€” Security Safeguards

1. Entities must apply security safeguards appropriate to the sensitivity of personal data, including:
Â Â Â Â (a) end-to-end encryption for data in transit;
Â Â Â Â (b) secure storage with access logging;
Â Â Â Â (c) multifactor authentication for employee access;
Â Â Â Â (d) anonymization or pseudonymization where possible.


2. Breaches must be reported to the Ombudsperson within 72 hours of discovery.



Regulation 5 â€” Compensation Framework

1. Where personal data is monetized, individuals must receive:
Â Â Â Â (a) a direct monetary share; or
Â Â Â Â (b) credit toward goods, services, or social benefit programs, at fair market value.


2. The Ombudsperson may issue binding guidelines for calculating fair value.



Regulation 6 â€” Whistleblower Protections

1. Whistleblowers are entitled to:
Â Â Â Â (a) confidentiality of identity;
Â Â Â Â (b) immunity from civil liability for disclosures made in good faith;
Â Â Â Â (c) reinstatement, compensation, and damages if retaliated against.



Regulation 7 â€” Foreign Transfer Certification

1. Any entity transferring personal data abroad must certify:
Â Â Â Â (a) the foreign jurisdictionâ€™s data protection laws;
Â Â Â Â (b) contractual safeguards ensuring equivalent protection to this Act;
Â Â Â Â (c) the right of individuals to withhold consent.


2. The Ombudsperson may prohibit transfers to jurisdictions deemed inadequate.



Regulation 8 â€” Emerging Technology Safeguards

1. New categories of technology not listed in the Schedule but posing material risks shall be regulated provisionally as high-risk.


2. Provisional status applies until a determination is made by Parliament or regulation under section 35.


-Full Version

Bill C-0101

An Act to recognize and protect the digital rights and freedoms of individuals in Canada, to establish ownership and fair value of personal data, and to provide transparency and accountability in digital practices


---

Preamble

Whereas Parliament affirms that personal data is an extension of individual autonomy and dignity;

Whereas the fair and transparent use of data is necessary to preserve democracy, human rights, and equality of opportunity;

And whereas it is expedient to establish a modern digital framework that prevents overreach, protects individuals against exploitation, and ensures accountability of both public and private actors;

Now, therefore, Her Majesty, by and with the advice and consent of the Senate and House of Commons of Canada, enacts as follows:


---

Short Title

Marginal note: Short title

1. This Act may be cited as the Canadian Digital Freedom and Rights Act.




---

Part I â€” Purpose and Interpretation

Marginal note: Purpose
2. The purpose of this Act is to guarantee individual ownership and control of personal data, to ensure fair sharing of value, to require transparency and accountability in digital practices, and to protect the dignity and autonomy of all individuals in Canada.

Marginal note: Definitions
3. The following definitions apply in this Act.
(a) â€œalgorithmic systemâ€ means any automated process, including artificial intelligence or machine learning, that uses personal data to inform or make decisions;
(b) â€œbiometric dataâ€ means data relating to physical, physiological or behavioural characteristics that uniquely identify an individual;
(c) â€œCommissionerâ€ means the Digital Rights Ombudsperson appointed under Part VII;
(d) â€œconsentâ€ means clear, informed, specific and freely given permission by an individual that may be withdrawn at any time without penalty or loss of essential services;
(e) â€œdark patternâ€ means any design or process intended to manipulate an individual into surrendering personal data or rights;
(f) â€œdata monetizationâ€ means any sale, licensing, transfer or commercial use of personal data;
(g) â€œentityâ€ means any government institution, corporation, partnership, association or person engaged in the processing of personal data;
(h) â€œneurodataâ€ means data derived from neurological activity or cognitive processes;
(i) â€œpersonal dataâ€ means any information relating to an identifiable individual, including identifiers, communications, metadata, biometric data, neurodata, preferences, reputations, behavioural patterns and histories;
(j) â€œprocessingâ€ means any operation performed on personal data, including collection, recording, analysis, storage, transfer, disclosure, deletion and monetization.


---

Part II â€” Rights of Individuals

Marginal note: Ownership of personal data
4. (1) Every individual is the sole owner of their personal data.
(2) No entity shall process personal data without the consent of the individual, except as required by law and subject to oversight under this Act.

Marginal note: Right to fair value
5. (1) No entity shall profit from personal data without providing fair compensation to the individual.
(2) Every individual is entitled to participate in any aggregate monetization scheme, including through a data dividend.

Marginal note: Right to transparency
6. (1) Every individual has the right, on request, to know in plain language
(a) what personal data is collected;
(b) by whom;
(c) for what purpose;
(d) for how long; and
(e) whether it has been monetized or transferred.
(2) No entity shall conceal its processing practices through complexity or obscurity.

Marginal note: Right to portability
7. (1) Every individual has the right to receive their personal data in a portable, standardized format.
(2) This right includes reputational value, transaction history and contribution records.

Marginal note: Right to erasure and expiry
8. (1) Every individual has the right to require deletion of their personal data.
(2) All personal data shall carry an expiry date after which it must be deleted unless consent is renewed.

Marginal note: Right to algorithmic fairness
9. (1) Every individual has the right to know when a decision is made or informed by an algorithmic system.
(2) No individual shall be subject to a decision based solely on automated processing without meaningful human review.
(3) Every individual may opt out of algorithmic processing where feasible.

Marginal note: Right to security and non-exploitation
10. (1) Every individual has the right to expect safeguards against unauthorized access.
(2) No entity shall process personal data for the purpose of exploiting vulnerabilities, including addiction, gambling or predatory targeting.

Marginal note: Right to non-discrimination
11. (1) No individual shall be denied services, employment or opportunities because they withhold personal data.
(2) No algorithmic system shall discriminate on grounds prohibited by the Canadian Human Rights Act.


---

Part III â€” Prohibited Practices

Marginal note: Prohibitions
12. No entity shall
(a) require the surrender of personal data as a condition of essential services;
(b) employ dark patterns to obtain consent;
(c) create or sell shadow profiles on non-users;
(d) retain personal data beyond its expiry without renewed consent;
(e) monetize personal data without disclosure and compensation;
(f) collect or process biometric data or neurodata without explicit, separate consent.


---

Part IV â€” Duties of Entities

Marginal note: Duty of transparency
13. Entities shall maintain public records of their processing practices in plain language.

Marginal note: Duty of minimal collection
14. Entities shall collect only personal data necessary for the stated purpose at the time of collection.

Marginal note: Duty of fair compensation
15. Entities shall provide mechanisms for individuals to receive value from data monetization, whether through direct payment, benefit credits or dividends.

Marginal note: Duty of breach notification
16. Entities shall notify affected individuals and the Commissioner without delay of any unauthorized access, disclosure or misuse.

Marginal note: Duty of algorithmic audit
17. Entities deploying high-risk algorithms shall be subject to independent third-party audits for fairness and bias.


---

Part V â€” Transparency and Oversight

Marginal note: Open data ledger
18. (1) All monetization of personal data shall be recorded in a secure, auditable open data ledger.
(2) Entities shall disclose the value assigned to personal data in each transaction.
(3) Individuals may access aggregate information regarding the use of their data.


---

Part VI â€” Special Protections

Marginal note: Biometric and neurodata
19. (1) Biometric data and neurodata are classified as highly sensitive.
(2) Their collection or processing requires explicit, separate consent.
(3) Such data shall not be monetized except through direct licensing by the individual.

Marginal note: Children
20. (1) The rights of minors under this Act shall be exercised by their guardians.
(2) On reaching the age of majority, an individual shall acquire full control of their personal data.
(3) Targeted advertising to minors is prohibited.


---

Part VII â€” Oversight and Remedies

Marginal note: Office established
21. (1) There is established the Office of the Digital Rights Ombudsperson.
(2) The Ombudsperson is independent and reports directly to Parliament.
(3) The Ombudsperson may investigate complaints, mediate disputes, issue binding orders, and publish audits.

Marginal note: Tribunal
21.1 (1) A Digital Rights Tribunal is hereby established to hear appeals from orders of the Ombudsperson.
(2) The Tribunal has all the powers of a superior court of record.
(3) Decisions of the Tribunal may be appealed to the Federal Court of Appeal on questions of law.

Marginal note: Individual remedies
22. (1) Any person whose rights under this Act are infringed is entitled to restitution, compensation, and correction of records.
(2) Class proceedings may be brought for systemic violations.
(3) Punitive damages may be awarded where violations are deliberate or reckless.

Marginal note: Whistleblower protections
22.1 (1) No person shall be disciplined, dismissed, or otherwise disadvantaged for disclosing a contravention of this Act.
(2) Whistleblowers are entitled to confidentiality and protection against retaliation.


---

Part VIII â€” Government and Public Sector Obligations

Marginal note: Equal application
23. This Act binds the Crown.

Marginal note: Limits on surveillance
24. (1) No government institution shall collect, retain, or use personal data except as expressly authorized by law and consistent with this Act.
(2) Mass surveillance of individuals without individualized lawful authority is prohibited.
(3) Emergency powers under other statutes shall not be interpreted to override this Act, except as expressly provided by Parliament.

Marginal note: Foreign transfers
25. (1) No personal data shall be transferred outside Canada without notice to the individual.
(2) Entities shall disclose the jurisdiction and applicable legal regime of the receiving state.
(3) Individuals may withhold consent to foreign transfer.

Marginal note: National security exceptions
26. (1) Nothing in this Act prevents disclosure required under the Security of Canada Information Disclosure Act or the Criminal Code.
(2) Such disclosures must be reported annually to Parliament in aggregate form, except where public disclosure would endanger national security.
(3) Oversight of national security disclosures rests with the National Security and Intelligence Review Agency.


---

Part IX â€” Enforcement and Penalties

Marginal note: Administrative penalties
27. (1) An entity that contravenes this Act is liable to an administrative monetary penalty not exceeding
(a) $10,000,000 or 2% of global gross revenue, whichever is greater, for a first contravention; and
(b) $25,000,000 or 5% of global gross revenue, whichever is greater, for a subsequent contravention.

Marginal note: Criminal offences
28. (1) Every person who knowingly
(a) monetizes personal data without consent,
(b) engages in mass surveillance contrary to section 24, or
(c) retaliates against a whistleblower,
is guilty of an offence punishable on conviction on indictment by a fine not exceeding $50,000,000 or imprisonment for a term not exceeding five years, or both.


---

Part X â€” Accountability and Review

Marginal note: Annual report
29. The Ombudsperson shall table an annual report in both Houses of Parliament setting out compliance rates, systemic issues, and recommendations.

Marginal note: Parliamentary review
30. (1) A comprehensive review of this Act shall be undertaken every five years by a committee of the House of Commons designated for that purpose.
(2) The committee shall report to Parliament with recommendations for amendment.

Marginal note: Sunset of exceptions
31. Any exception to this Act enacted under section 26 shall expire five years after its enactment unless renewed by Parliament.


---

Part XI â€” General

Marginal note: Liberal interpretation
32. This Act shall be interpreted broadly and liberally in favour of the freedom, dignity, and autonomy of individuals.

Marginal note: Supremacy of Act
33. In the event of conflict with another Act of Parliament, this Act prevails to the extent of the conflict, except where the Canadian Charter of Rights and Freedoms otherwise provides.


---

Part XII â€” Future-Proofing Digital Rights

Marginal note: Technological neutrality
34. This Act applies to all technologies, platforms, and processing methods, whether existing or developed in the future.

Marginal note: Emerging technologies
35. (1) The Governor in Council may, on the recommendation of the Ombudsperson and following consultation with Parliament, make regulations designating new categories of high-risk algorithmic systems subject to this Act.
(2) Designated systems shall be added to the Schedule.
(3) The designation of a new category shall be reviewed within three years of its addition.

Marginal note: Precautionary principle
36. Where doubt exists as to the safety, fairness, or ethical impact of an algorithmic system, it shall be regulated as high-risk until proven otherwise.

Marginal note: Independent advisory panel
37. (1) There is established a Digital Futures Advisory Panel, composed of experts in technology, law, human rights, and ethics.
(2) The Panel shall publish biennial reports on emerging risks and recommend amendments to this Act.


---

Schedule â€” High-Risk Algorithmic Systems

(Section 35)

The following categories of algorithmic systems are designated as high-risk and subject to enhanced obligations under this Act:

1. Biometric recognition systems â€” including facial, voice, gait, and iris recognition.


2. Predictive policing and surveillance systems â€” including crime prediction, risk scoring, and automated surveillance analytics.


3. Employment and labour systems â€” including algorithmic hiring, performance evaluation, and termination decisions.


4. Credit, insurance, and financial profiling systems â€” including automated credit scoring, loan approval, and premium calculation.


5. Health and genetic systems â€” including predictive health analytics, genetic risk scoring, and treatment recommendation engines.


6. Education and child profiling systems â€” including automated grading, behavioural monitoring, and opportunity allocation.


7. Content recommendation and behavioural manipulation systems â€” including algorithmic feeds, political micro-targeting, and addictive engagement loops.


8. Neurotechnology systems â€” including brainâ€“computer interfaces, cognitive monitoring, and emotional recognition.


9. Any system designated by regulation under section 35.




---

Annex â€” Draft Regulations

Pursuant to section 35 of the Canadian Digital Freedom and Rights Act


---

Regulation 1 â€” Algorithmic Audit Standards

1. Every audit of a high-risk algorithmic system shall include:
(a) a description of the system, its purpose, and decision-making process;
(b) the categories of personal data used;
(c) the criteria, variables, or proxies influencing outputs;
(d) testing for bias, discrimination, and disparate impact on protected groups;
(e) a record of error rates and reliability measures;
(f) procedures for human review and override;
(g) a statement of compliance with the Canadian Human Rights Act.


2. Audits must be conducted at least annually by an independent, accredited third party.




---

Regulation 2 â€” Plain-Language Disclosures

1. Disclosures under section 6 of the Act must be written in plain language, not exceeding a Grade 8 reading level.


2. A disclosure must include:
(a) the name and contact information of the entity;
(b) the purpose of data collection;
(c) the categories of data collected;
(d) whether the data will be monetized, shared, or transferred abroad;
(e) the expiry date of the data;
(f) the rights of the individual under this Act.


3. No disclosure is valid if presented in a manipulative or coercive design (dark pattern).




---

Regulation 3 â€” Data Ledger Reporting

1. Entities monetizing personal data must submit quarterly reports to the Ombudsperson containing:
(a) total volume of data monetized;
(b) categories of data monetized;
(c) aggregate value received;
(d) proportion of value distributed to individuals;
(e) identity of third-party recipients.


2. Reports must be published in an anonymized public ledger accessible online.




---

Regulation 4 â€” Security Safeguards

1. Entities must apply security safeguards appropriate to the sensitivity of personal data, including:
(a) end-to-end encryption for data in transit;
(b) secure storage with access logging;
(c) multifactor authentication for employee access;
(d) anonymization or pseudonymization where possible.


2. Breaches must be reported to the Ombudsperson within 72 hours of discovery.




---

Regulation 5 â€” Compensation Framework

1. Where personal data is monetized, individuals must receive:
(a) a direct monetary share; or
(b) credit toward goods, services, or social benefit programs, at fair market value.


2. The Ombudsperson may issue binding guidelines for calculating fair value.




---

Regulation 6 â€” Whistleblower Protections

1. Whistleblowers are entitled to:
(a) confidentiality of identity;
(b) immunity from civil liability for disclosures made in good faith;
(c) reinstatement, compensation, and damages if retaliated against.




---

Regulation 7 â€” Foreign Transfer Certification

1. Any entity transferring personal data abroad must certify:
(a) the foreign jurisdictionâ€™s data protection laws;
(b) contractual safeguards ensuring equivalent protection to this Act;
(c) the right of individuals to withhold consent.


2. The Ombudsperson may prohibit transfers to jurisdictions deemed inadequate.




---

Regulation 8 â€” Emerging Technology Safeguards

1. New categories of technology not listed in the Schedule but posing material risks shall be regulated provisionally as high-risk.


2. Provisional status applies until a determination is made by Parliament or regulation under section 35.
